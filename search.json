[{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://lsteinmann.github.io/datplot/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"the-inscriptions-of-bithynia-data-set","dir":"Articles","previous_headings":"","what":"The “Inscriptions of Bithynia” data set","title":"Data Preparation and Visualization","text":"manually curated data B. Weissova prepared dissertation saved Excel-spreadsheet. order comply CRAN, slightly reformatted csv-file containing ASCII-characters. continue file, available repositories “inst/extdata/”-sub-directory. can loaded R without complications using read.csv()-function. Please note: Since conversion original Excel file CRAN-compatible ASCII-data script convert data anymore. apologize updating script, seems irrelevant spent time already , since package contains clean Data already. keeping lesson data complicated, things change. large amount initial data provided Searchable Greek Inscriptions Tool Packard Humanities Institute Epigraphische Datenbank Heidelberg. inscriptions (n = 0) can referenced databases ikey. data set supplemented 0 inscriptions manually gathered different sources. relevant citations found Source column. Additional information creation curation data set can found publication mentioned (Weissova 2019). original file consists five columns row representing single inscription: ikey contains reference Searchable Greek Inscriptions Tool Packard Humanities Institute, indicated via PH prefix, Epigraphische Datenbank Heidelberg, indicated via HD prefix. Location refers find spot inscription. Source states source data. Chronological Frame contains dating verbose format, “Roman Imperial Period”. Language records language inscription written, can either Latin, Greek, .","code":"inscriptions <- read.csv(system.file(\"extdata\",                                      \"Bithynia_Inscriptions_ascii.csv\",                                      package = \"datplot\")) summary(inscriptions) #>        X              ikey             Location            Source          #>  Min.   :   1.0   Length:2878        Length:2878        Length:2878        #>  1st Qu.: 720.2   Class :character   Class :character   Class :character   #>  Median :1439.5   Mode  :character   Mode  :character   Mode  :character   #>  Mean   :1439.5                                                            #>  3rd Qu.:2158.8                                                            #>  Max.   :2878.0                                                            #>  Chronological.Frame   Language         #>  Length:2878         Length:2878        #>  Class :character    Class :character   #>  Mode  :character    Mode  :character   #>                                         #>                                         #>"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"data-preparation-cleaning-and-reformatting","dir":"Articles","previous_headings":"","what":"Data Preparation, Cleaning and Reformatting","title":"Data Preparation and Visualization","text":"data set yet suited analysis, variables, especially chronological frame, many inconsistencies. processing, also sure include identifier-column. 331 inscriptions ikey-Value, might otherwise good candidate identification, chose create new automatically generated ID, every inscription can individually identifiable. Two variables data set almost ready use, .e. Location Language. look unique values reveals small inconsistencies can easily fixed: Using functions tidyverse package family, can easily transform columns. rename “Chronological Frame” Dating, shorter names without spaces convenient work , add proper NA-values chronological assessment. mutate() replace() also clear redundant variable values Location Language: conversion leaves us compact overview data sets contents:","code":"inscriptions$ID <- paste(\"I_\", seq_len(nrow(inscriptions)), sep = \"\") unique(inscriptions$Location) #>  [1] \"Apamea\"                    \"Apollonia ad Rhyndacum\"    #>  [3] \"Caesarea Germanice\"        \"Chalcedon\"                 #>  [5] \"Claudiopolis\"              \"Creteia Flaviopolis\"       #>  [7] \"Daskyleion\"                \"Heraclea Pontica\"          #>  [9] \"Heraclea Pontica \"         \"Juliopolis\"                #> [11] \"Kaisarea Hadrianopolis\"    \"Nicaea\"                    #> [13] \"Nicaea \"                   \"Nicomedia\"                 #> [15] \"Nicomedia (Dakibyza)\"      \"Prusias ad Hypium\"         #> [17] \"Prusias ad Mare\"           \"Prusias ad Mare (Keramed)\" #> [19] \"Prusias ad Olympum\"        \"Pylai\"                     #> [21] \"Strobilos\"                 \"Tios\"                      #> [23] \"Tios \"                     \"unknown\"                   #> [25] \"unknown (Bolu museum)\"     \"unknown (Lamounia ?)\" unique(inscriptions$Language) #> [1] \"Greek\"    \"Gr/Lat\"   \"Latin\"    \"Gr / Lat\" inscriptions <- inscriptions %>%   rename(Dating = Chronological.Frame) %>%   mutate(Dating = na_if(Dating, \"---\"),          Language = replace(Language, Language == \"Gr/Lat\", \"Greek/Latin\"),          Language = replace(Language, Language == \"Gr / Lat\", \"Greek/Latin\"),          Language = factor(Language, levels = c(\"Greek\", \"Latin\",                                                 \"Greek/Latin\")),          Location = replace(Location, str_detect(Location, \"unknown\"),                             \"unknown\"),          Location = replace(Location,                             Location == \"Prusias ad Mare (Keramed)\",                             \"Prusias ad Mare\"),          Location = factor(Location)) summary(inscriptions) #>        X              ikey                         Location   #>  Min.   :   1.0   Length:2878        Nicaea            :760   #>  1st Qu.: 720.2   Class :character   Nicomedia         :501   #>  Median :1439.5   Mode  :character   Prusias ad Olympum:344   #>  Mean   :1439.5                      Claudiopolis      :252   #>  3rd Qu.:2158.8                      Chalcedon         :200   #>  Max.   :2878.0                      Prusias ad Hypium :179   #>                                      (Other)           :642   #>     Source             Dating                 Language         ID            #>  Length:2878        Length:2878        Greek      :2724   Length:2878        #>  Class :character   Class :character   Latin      : 125   Class :character   #>  Mode  :character   Mode  :character   Greek/Latin:  29   Mode  :character   #>                                                                              #>                                                                              #>                                                                              #>"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"cleaning-up-the-dating-variable","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting","what":"Cleaning up the Dating-variable","title":"Data Preparation and Visualization","text":"values Dating-variable contain question signs, indicating uncertainty chronological frames assessment. keep information, store new variable uncertain_dating, contains TRUE question mark original assessment FALSE dating certain, one wishes additional information later used select exclude uncertain values.","code":"inscriptions$uncertain_dating <- FALSE sel <- grep(\"\\\\?\", inscriptions$Dating) inscriptions$uncertain_dating[sel] <- TRUE inscriptions$Dating <- gsub(\"\\\\?\", \"\", inscriptions$Dating)"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"creating-a-concordance-for-periods","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting > Cleaning up the Dating-variable","what":"Creating a Concordance for Periods","title":"Data Preparation and Visualization","text":"next step sorting values Dating variable manually entered, “Roman Imperial Period”. achieve excluding values contain number, preparing table can manually enter desired dating span saving .csv-file. adding values manually, reload file. manually edited concordance can found “inst/extdata/”-sub-directory repository file periods_edit.csv corresponds chronological assessment periods used original publication (Weissova 2019, 42). course possible add corresponding values R. Since process hardly automated, way seemed efficient us, though - sadly - less transparent vignette. values can examined reader csv-Table mentioned , loading table via system.file() seen code chunk.","code":"sel <- grepl(\"[0-9]\", inscriptions$Dating) periods <- data.frame(\"Dating\" = unique(inscriptions$Dating[which(sel == FALSE)])) periods$DAT_min <- NA periods$DAT_max <- NA #write.csv(periods, file = \"../data-raw/periods.csv\", fileEncoding = \"UTF-8\") # .... Manual editing of the resulting table, saving it as \"periods_edit.csv\". join_dating <- read.csv(file = system.file('extdata', 'periods_edit.csv',                                            package = 'datplot',                                            mustWork = TRUE),                         row.names = 1,                         colClasses = c(\"character\", \"character\",                                        \"integer\", \"integer\"),                         encoding = \"UTF-8\")"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"reformatting-of-partially-numerical-dating-values","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting > Cleaning up the Dating-variable","what":"Reformatting of Partially Numerical Dating Values","title":"Data Preparation and Visualization","text":"remains, however, large amount values covered concordance. can easily automate conversions series steps store another data.frame called num_dating, encompassing unique values contain form numerical dating. First, number inscriptions dated single year. select using regular expression grep()1 can simply delete character-part values specified year remains stored DAT_-variables. separately AD BC-values since BC needs stored negative number. demonstration, resulting table (num_dating) point converted values: Since frequently check values num_dating look errors, append finished rows join_dating, later use look--table data set, remove finished rows num_dating. convert Dating variable factor character now, can use strsplit()-function values: values format year-year, e.g. “150-160 AD”, can easily grab numbers string. achieve , select rows containing relevant format, loop rows split character string along either “-” spaces, later “/”, chose treat values format “198/198 AD” “197 - 198 AD”. Selecting numerical values resulting list according position gives us desired values DAT_min DAT_max. need BC-values make sure return negative numbers. Another look data set can help us check possible errors. Putting aside finished values makes easier spot errors process:","code":"num_dating <- data.frame(\"Dating\" = unique(inscriptions$Dating[which(sel == TRUE)])) num_dating$DAT_min <- NA num_dating$DAT_max <- NA sel <- grep(\"^[0-9]{1,3} AD$\", num_dating$Dating) num_dating$DAT_min[sel] <- gsub(\" AD\", \"\", num_dating$Dating[sel])  num_dating$DAT_max[sel] <- gsub(\" AD\", \"\", num_dating$Dating[sel])  sel <- grep(\"^[0-9]{1,3} BC$\", num_dating$Dating) num_dating$DAT_min[sel] <- paste(\"-\", gsub(\" BC\", \"\", num_dating$Dating[sel]),                                   sep = \"\") num_dating$DAT_max[sel] <- paste(\"-\", gsub(\" BC\", \"\", num_dating$Dating[sel]),                                   sep = \"\") join_dating <- rbind(join_dating, num_dating[!is.na(num_dating$DAT_min), ]) num_dating <- num_dating[which(is.na(num_dating$DAT_min)), ] num_dating$Dating <- as.character(num_dating$Dating) # Values like: 92-120 AD sel <- grep(\"^[0-9]{1,3}-[0-9]{1,3} AD\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"-| \")   num_dating$DAT_min[r] <- split[[1]][1]   num_dating$DAT_max[r] <- split[[1]][2] } # Values like: AD 92-120 sel <- grep(\"^AD [0-9]{1,3}-[0-9]{1,3}$\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"-| \")   num_dating$DAT_min[r] <- split[[1]][2]   num_dating$DAT_max[r] <- split[[1]][3] } # Values like: AD 92 - 120 sel <- grep(\"^AD [0-9]{1,3} - [0-9]{1,3}\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \" - | \")   num_dating$DAT_min[r] <- split[[1]][2]   num_dating$DAT_max[r] <- split[[1]][3] } # Values like: 198/199 AD sel <- grep(\"^[0-9]{1,3}/[0-9]{1,3} AD\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"/| \")   num_dating$DAT_min[r] <- split[[1]][1]   num_dating$DAT_max[r] <- split[[1]][2] } # Values like: 525-75 BC sel <- grep(\"^[0-9]{1,3}-[0-9]{1,3} BC\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"-| \")   num_dating$DAT_min[r] <- 0 - as.numeric(split[[1]][1])   num_dating$DAT_max[r] <- 0 - as.numeric(split[[1]][2]) } join_dating <- rbind(join_dating, num_dating[!is.na(num_dating$DAT_min), ]) num_dating <- num_dating[which(is.na(num_dating$DAT_min)), ]"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"reformatting-of-inscriptions-dated-to-complete-centuries","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting > Cleaning up the Dating-variable","what":"Reformatting of Inscriptions Dated to Complete Centuries","title":"Data Preparation and Visualization","text":"Next, separate values identify complete centuries. want express dating absolute numbers, convert “1st c. AD” time span ranging 0 99, “1st c. BC” -99 0 respectively. regular expression selects values, single number beginning string followed two letters (.e. 2nd, 3rd, 1st) “c. AD” resp. “c. BC”. subtract one multiply number 100 get respective boundaries, taking care treat AD BC differently. , putting aside finished values makes easier spot errors processing:","code":"sel <- grep(\"^[0-9]{1}[a-z]{2} c\\\\. AD$\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"[a-z]{2} c\\\\.\")   split <- as.numeric(split[[1]][1])   num_dating$DAT_min[r] <- ((split - 1) * 100)   num_dating$DAT_max[r] <- ((split - 1) * 100) + 99 }  sel <- grep(\"^[0-9]{1}[a-z]{2} c\\\\. BC$\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \"[a-z]{2} c\\\\.\")   split <- as.numeric(split[[1]][1])   num_dating$DAT_min[r] <- 0-(split * 100) + 1   num_dating$DAT_max[r] <- 0-((split - 1) * 100) } join_dating <- rbind(join_dating, num_dating[!is.na(num_dating$DAT_min), ]) num_dating <- num_dating[which(is.na(num_dating$DAT_min)), ]"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"reformatting-of-imprecise-dating-values","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting > Cleaning up the Dating-variable","what":"Reformatting of Imprecise Dating Values","title":"Data Preparation and Visualization","text":"dates around certain value, .e. format “ca. 190 AD”, able make informed decision guess researchers providing assessment mind. might also change inscription inscription. closer look individual inscriptions may yield sensible estimate, seems feasible part data preparation process. seems cases, dating can precise span around 10 years, researchers tend emphasize . Therefore, decided take total span 20 years, .e. 10 years 10 years mentioned value, reflecting uncertainty precision duration estimate. “ca. 190 AD” thus becomes “180–200 AD”, mechanism BC negative values.","code":"sel <- grep(\"^ca\\\\. [0-9]{1,3} AD$\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \" \")   split <- as.numeric(split[[1]][2])   num_dating$DAT_min[r] <- split - 10   num_dating$DAT_max[r] <- split + 10 } sel <- grep(\"^ca\\\\. [0-9]{1,3} BC$\", num_dating$Dating) for (r in sel) {   split <- strsplit(x = num_dating$Dating[r], split = \" \")   split <- 0-as.numeric(split[[1]][2])   num_dating$DAT_min[r] <- split - 10   num_dating$DAT_max[r] <- split + 10 }"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"creating-a-second-concordance-for-verbose-chronological-assessments","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting > Cleaning up the Dating-variable","what":"Creating a Second Concordance for Verbose Chronological Assessments","title":"Data Preparation and Visualization","text":", saving finished values join_dating leaves us list yet converted values seen num_datings Dating-variable. Due heterogeneous nature 101 values, decided convert manually . criteria translating terms like “beginning ” following: “beginning ”, “end ”, “early”, “late” – similarly ca. – translated 20 years, assume information given time span greater quarter century. values, employed criteria seen automated process. decided measure ‘beginning century’ e.g. -199 BC values 100 AD values, accordingly identify ‘end century’ values e.g. -100 (BC) / 199 (AD). Since data epigraphical, case “” “” (.e. terminus ante/post quem) dates, assume connection datable event, therefore add subtract span 10 years, , however, still rather arbitrary. last step, switch – cases automatically assigned – year 0, treated either 1 -1: reload corrected data. values put aside join_dating data.frame beforehand serve basis new data.frame use look-table. variables treated character-strings, need convert numeric format first. append newly loaded manually assigned values data.frame. , keep process transparent possible, included *.csv-Table “inst/extdata/” sub-directory “num_dating_edit.csv”.","code":"join_dating <- rbind(join_dating, num_dating[!is.na(num_dating$DAT_min), ]) num_dating <- num_dating[which(is.na(num_dating$DAT_min)), ] unique(num_dating$Dating)[1:20] #>  [1] \"end of the 2nd c. AD\"                 #>  [2] \"end of 1st c. BC - beg. of 1st c. AD\" #>  [3] \"end 2nd c. BC\"                        #>  [4] \"early 1st c. AD\"                      #>  [5] \"after 212 AD\"                         #>  [6] \"AD 531\"                               #>  [7] \"5th/6th century AD\"                   #>  [8] \"3rd/4th century AD\"                   #>  [9] \"2nd - 3rd c. AD\"                      #> [10] \"2nd - 1st c. BC\"                      #> [11] \"1st - beg. of the 2nd c. AD\"          #> [12] \"1st - 2nd c. AD\"                      #> [13] \"AD 117-138 \"                          #> [14] \"2nd century AD\"                       #> [15] \"late 3rd - early 2nd c. BC\"           #> [16] \"5th - 6th c. AD\"                      #> [17] \"3rd quarter of the 6th c. BC\"         #> [18] \"3rd - 2nd c. BC\"                      #> [19] \"340/339 BC\"                           #> [20] \"27 BC-14 AD\" join_dating$DAT_min[which(join_dating$DAT_min == 0)] <- 1 join_dating$DAT_max[which(join_dating$DAT_max == 0)] <- -1 #write.csv(num_dating, file = \"../data-raw/num_dating.csv\",  #          fileEncoding = \"UTF-8\") num_dating <- read.csv(file = system.file('extdata', 'num_dating_edit.csv',                                           package = 'datplot', mustWork = TRUE),                         encoding = \"UTF-8\",                        row.names = 1,                        colClasses = c(\"character\", \"character\",                                       \"integer\", \"integer\")) join_dating <- join_dating %>%   mutate(DAT_min = as.integer(DAT_min),          DAT_max = as.integer(DAT_max)) %>%   rbind(num_dating)"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"joining-the-reformatted-data-with-the-original-data-set","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting","what":"Joining the Reformatted Data with the Original Data Set","title":"Data Preparation and Visualization","text":"left_join() lets us add DAT_-variables look-table join_dating original data.frame. DAT_-variables – can now see – contain desired information:","code":"inscriptions <- left_join(inscriptions, join_dating, by = \"Dating\")"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"fixing-mistakes","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting","what":"Fixing mistakes","title":"Data Preparation and Visualization","text":"can now make ‘test run’ using datplot stepsize-value set 5 order save much time possible2 check errors first. select exactly 4 columns correct order : Identifier, Grouping Variable, Minimum Dating, Maximum Dating. transform output pipe operator data.frame handing datstepts(), datplot needs format. indeed problems data. first three warnings issued datsteps() informs us may assigned values wrong: “Warning: Dating seems wrong order ID I_1162, I_2725 (Index: 637, 1458). Dates switched, sure check original data possible mistakes.” dates wrong order, datplot automatically switch proceeding. , however, might always correct way handling situation, errors might occurred well. Therefore, check data using Index-Values IDs provided datplots warning: problem twofold. first entry, automated translation dating handle format “62/3” thus returned wrong value DAT_max. truly large data set, correct original reformatting process. case, efficient fix problem right now fastest possible way. case, Dating-column provided BC-values wrong order. can also fix quickly right , basically way datplot handle cases internally: Note datsteps() changes date proceeds, warns possibility errors. therefore recommended check data mentioned warning. ‘fixed’ data set, save , error occur twice:","code":"inscr_steps <- inscriptions %>%   select(ID, Location, DAT_min, DAT_max) %>%   na.omit() %>%   as.data.frame() %>%   datplot::datsteps(stepsize = 5) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> Warning in switch.dating(DAT_df): Warning: Dating seems to be in wrong order at #> ID I_1162, I_2725 (Index: 599, 1269). Dates have been switched, but be sure to #> check your original data for possible mistakes. #> Warning in get.weights(DAT_mat[, \"datmin\"], DAT_mat[, \"datmax\"]): Warning: #> DAT_min and DAT_max at Index: 57, 68, 120, 173, 187, 238, 299, 300, 311, 312, #> 588, 590, 679, 794, 798, 799, 828, 831, 833, 834, 837, 841, 878, 879, 908, 909, #> 914, 915, 931, 932, 933, 937, 938, 941, 942, 997, 1051, 1064, 1067, 1130, 1307, #> 1308, 1310, 1322, 1323, 1324 have the same value! Is this correct? Please check #> the table for possible errors. #> Warning in create.sub.objects(DAT_list, stepsize, calc, cumulative): stepsize #> is larger than the range of the closest dated object at Index = 39, 43, 44, 57, #> 68, 69, 70, 101, 107, 112, 113, 114, 120, 123, 129, 146, 170, 172, 173, 175, #> 177, 178, 179, 187, 190, 212, 214, 215, 238, 261, 262, 292, 298, 299, 300, 301, #> 302, 303, 304, 305, 306, 308, 310, 311, 312, 313, 314, 315, 318, 319, 320, 588, #> 590, 602, 606, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 794, #> 796, 797, 798, 799, 800, 801, 803, 804, 805, 807, 808, 827, 828, 831, 832, 833, #> 834, 835, 836, 837, 838, 839, 840, 841, 847, 848, 849, 878, 879, 902, 908, 909, #> 910, 911, 912, 913, 914, 915, 917, 920, 921, 922, 923, 929, 931, 932, 933, 934, #> 935, 936, 937, 938, 941, 942, 943, 948, 949, 989, 995, 997, 998, 999, 1000, #> 1003, 1004, 1009, 1011, 1016, 1036, 1051, 1053, 1054, 1062, 1064, 1067, 1129, #> 1130, 1133, 1267, 1290, 1297, 1302, 1306, 1307, 1308, 1310, 1322, 1323, 1324). #> For information see documentation of get.step.sequence(). inscriptions %>%   select(ID, Location, Dating, uncertain_dating, DAT_min, DAT_max) %>%   na.omit() %>%   slice(637, 1458) %>%   kable() inscriptions[which(inscriptions$ID == \"I_1162\"),\"DAT_max\"] <- 63 inscriptions[which(inscriptions$ID == \"I_2725\"),c(\"DAT_min\", \"DAT_max\")] <-   inscriptions[which(inscriptions$ID == \"I_2725\"),c(\"DAT_max\", \"DAT_min\")] #write.table(inscriptions, file = \"../data-raw/inscriptions.csv\",  #            fileEncoding = \"UTF-8\", sep = \";\", row.names = FALSE)"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"storing-the-prepared-data-set","dir":"Articles","previous_headings":"Data Preparation, Cleaning and Reformatting","what":"Storing the Prepared Data Set","title":"Data Preparation and Visualization","text":"later use publication, add explanations variables metadata R-object store *.rda file. backup, also save finished table .csv-file, suitable archiving. .csv-Table, however, contain additional information added attributes. files available repository part package use researchers. kindly ask cite Weissova (2019), Packard Humanities Institute repository https://github.com/lsteinmann/datplot sources. dataset can loaded R simply calling datplot loaded.","code":"#library(datplot) data(\"Inscr_Bithynia\")"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"selecting-the-data-for-further-analysis-and-using-datplot","dir":"Articles","previous_headings":"","what":"Selecting the Data for Further Analysis and using datplot","title":"Data Preparation and Visualization","text":"aim get overview spatio-temporal distribution inscriptions Bithynia, can analyze inscriptions known Location Dating. Thus, remove rows data set contain information. means removed total 1380 rows, contain information needed. data set suitable analysis can summarized follows:","code":"inscr_clean <- Inscr_Bithynia %>%   filter(Dating != \"NA\",          Location != \"unknown\") %>%   droplevels() summary(inscr_clean) #>       ID                ikey                             Location   #>  Length:1498        Length:1498        Nicaea                :604   #>  Class :character   Class :character   Prusias ad Olympum    :279   #>  Mode  :character   Mode  :character   Nicomedia             :108   #>                                        Kaisarea Hadrianopolis: 87   #>                                        Apamea                : 77   #>                                        Chalcedon             : 69   #>                                        (Other)               :274   #>     Source             Dating                 Language    uncertain_dating #>  Length:1498        Length:1498        Greek      :1392   Mode :logical    #>  Class :character   Class :character   Latin      :  82   FALSE:1327       #>  Mode  :character   Mode  :character   Greek/Latin:  24   TRUE :171        #>                                                                            #>                                                                            #>                                                                            #>                                                                            #>     DAT_min          DAT_max           URL            #>  Min.   :-599.0   Min.   :-525.0   Length:1498        #>  1st Qu.: -31.0   1st Qu.: 199.0   Class :character   #>  Median : 100.0   Median : 266.5   Mode  :character   #>  Mean   : 100.3   Mean   : 286.4                      #>  3rd Qu.: 200.0   3rd Qu.: 395.0                      #>  Max.   : 680.0   Max.   : 799.0                      #>"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"datplot","dir":"Articles","previous_headings":"","what":"datplot","title":"Data Preparation and Visualization","text":"can now begin using datplot-package… …can actually try use datsteps(), first stepsize 25 years: datsteps() tells us number objects dated one year, asking us correct. included output avoid errors occurring faulty values, objects dated one year high impact outcome. might help spot problems discrepancies datasets well, warning order dating already shown. second step, scale weights according grouping variable (Location), sum weights group equals 1, important displaying weight correctly density plot.","code":"library(datplot) inscr_steps <- inscr_clean %>%   select(ID, Location, DAT_min, DAT_max) %>%   as.data.frame() %>%   datplot::datsteps(stepsize = 25) %>%   datplot::scaleweight(var = 2) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> Warning in get.weights(DAT_mat[, \"datmin\"], DAT_mat[, \"datmax\"]): Warning: #> DAT_min and DAT_max at Index: 60, 75, 141, 199, 214, 269, 334, 335, 347, 348, #> 625, 628, 637, 730, 879, 883, 884, 936, 939, 941, 942, 945, 949, 987, 988, #> 1022, 1023, 1028, 1029, 1046, 1047, 1048, 1052, 1053, 1057, 1059, 1114, 1172, #> 1185, 1188, 1189, 1256, 1496, 1497, 1498 have the same value! Is this correct? #> Please check the table for possible errors. #> Warning in create.sub.objects(DAT_list, stepsize, calc, cumulative): stepsize #> is larger than the range of the closest dated object at Index = 6, 12, 14, 18, #> 19, 20, 21, 22, 39, 40, 41, 44, 45, 60, 74, 75, 76, 77, 79, 80, 83, 110, 113, #> 114, 125, 126, 131, 132, 133, 141, 145, 147, 154, 161, 162, 163, 167, 168, 171, #> 173, 174, 175, 194, 196, 198, 199, 201, 203, 204, 205, 206, 207, 208, 212, 214, #> 217, 218, 223, 231, 232, 233, 234, 235, 236, 237, 238, 240, 242, 243, 244, 245, #> 246, 265, 269, 271, 272, 273, 276, 292, 293, 323, 324, 331, 332, 333, 334, 335, #> 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, #> 353, 354, 355, 356, 579, 582, 583, 584, 585, 617, 618, 619, 620, 621, 622, 623, #> 625, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 640, 646, 723, 724, 725, #> 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 873, 874, 875, 876, 877, #> 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, #> 935, 936, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, #> 952, 953, 954, 955, 956, 957, 958, 959, 975, 976, 979, 980, 981, 982, 983, 984, #> 987, 988, 989, 1002, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, #> 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, #> 1038, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, #> 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1074, 1075, 1078, #> 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1106, 1107, 1112, 1113, #> 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, #> 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1147, 1148, 1149, 1153, 1154, 1172, #> 1174, 1175, 1176, 1183, 1185, 1186, 1187, 1188, 1189, 1218, 1219, 1220, 1221, #> 1222, 1223, 1224, 1248, 1250, 1251, 1252, 1253, 1255, 1256, 1260, 1382, 1456, #> 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1480, #> 1487, 1494, 1496, 1497, 1498). For information see documentation of #> get.step.sequence()."},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"visualizing-the-output-of-datsteps","dir":"Articles","previous_headings":"","what":"Visualizing the Output of datsteps()","title":"Data Preparation and Visualization","text":"get general impression data set possibilities visualization, explore recommend different plot methods . simplest fastest way plot output base R plot() function combination density() function:","code":"plot(density(inscr_steps$DAT_step))"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"using-ggplot2","dir":"Articles","previous_headings":"Visualizing the Output of datsteps()","what":"Using ggplot2","title":"Data Preparation and Visualization","text":"somewhat crowded overview containing Locations Inscriptions can easily achieved using ggplot2 geom_density()-method, based procedure density()-function used . result may look like :  Note output without weights calculated datsteps() different:  weights calculated datplot (internally using get.weights()-function) way assigning importance objects question. relate range object dated (detailed information process available (Weissova Steinmann 2021)). Therefore, objects dated large time spans example “Roman Imperial Period” ranging 31 BC 395 AD data set, contributing curve significantly less object dated one exact year. times can useful look outputs discuss separately. look case study single city later clarify .","code":"ggplot(data = inscr_steps, aes(x = DAT_step, fill = variable,                                weight = weight)) +   geom_density(alpha = 0.3) #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights' ggplot(data = inscr_steps, aes(x = DAT_step, fill = variable)) +   geom_density(alpha = 0.3)"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"using-ggplot2-and-ggridges","dir":"Articles","previous_headings":"Visualizing the Output of datsteps()","what":"Using ggplot2 and ggridges","title":"Data Preparation and Visualization","text":"Since graph crowded layout, actually recommend using geom_density_ridges ggridges-package. Since package built support assigning weight, use stat = \"density\", reference density()-function get appropriate calculations: Styling help make plots readable:","code":"ggplot(data = inscr_steps,        aes(x = DAT_step,            y = fct_rev(as_factor(variable)),            fill = variable,            weight = weight)) +   geom_density_ridges(aes(height = after_stat(density)),                       stat = \"density\", alpha = 0.9) +   scale_fill_discrete(guide = FALSE) #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights' #> Warning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in #> ggplot2 3.3.4. #> ℹ Please use \"none\" instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. bluegreen <- colorRampPalette(c(\"#8dae25\", \"#17365c\"))  ggplot(data = inscr_steps,        aes(x = DAT_step,            y = fct_rev(as_factor(variable)),            fill = variable,            weight = weight)) +   geom_density_ridges(aes(height = after_stat(density)), stat = \"density\", alpha = 0.9) +   scale_x_continuous(breaks = seq(from = -800, to = 800, by = 100),                      limits = c(-800,800), name = \"\") +   geom_vline(xintercept = 0, alpha = 0.5, lwd = 1) +   theme(axis.text.x = element_text(angle = 90, vjust = 0.5),         panel.background = element_blank(),         panel.grid.major.x = element_line(linetype = \"dashed\",                                           color = \"gray30\"),         panel.grid.minor.x = element_line(linetype = \"dotted\",                                           color = \"gray80\")) +   scale_fill_manual(guide=FALSE,                     values = bluegreen(length(unique(inscr_steps$variable)))) +   labs(title = \"Epigraphic Evidence from Bithynia\",        subtitle = \"Spatio-temporal distribution\",        y = \"administrative centres\",        caption = attributes(inscriptions)$source) #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"using-ggplot2-and-facet_wrap","dir":"Articles","previous_headings":"Visualizing the Output of datsteps()","what":"Using ggplot2 and facet_wrap()","title":"Data Preparation and Visualization","text":"Another option separate variables facet_wrap, condensed support option scale densities graph maximum visibility, geom_density_ridges() automatically (see Documentation ggridges). case, leads worse visibility smaller density curves:  Styling also helps much improve graphs readability:","code":"ggplot(data = inscr_steps, aes(x = DAT_step,                                 fill = variable, weight = weight)) +   geom_density(alpha = 0.9) +   scale_fill_discrete(guide = FALSE) +   facet_wrap(variable ~ ., ncol = 1) #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights' ggplot(data = inscr_steps, aes(x = DAT_step,                                fill = variable, weight = weight)) +   geom_density(alpha = 0.9) +   theme(panel.background = element_blank()) +   scale_fill_manual(guide=FALSE,                     values = bluegreen(length(unique(inscr_steps$variable)))) +   scale_x_continuous(breaks = seq(from = -800, to = 800, by = 100),                      limits = c(-800,800), name = \"\") +   facet_wrap(variable ~ ., ncol = 1) +   theme(strip.text.x = element_text(size=8),         strip.background = element_blank(),         axis.text.x = element_text(angle = 90, vjust = 0.5),         axis.text.y = element_blank(),         axis.ticks.y = element_blank(),         axis.title.y = element_blank(),         panel.background = element_blank(),         panel.grid.major.x = element_line(linetype = \"dashed\",                                           color = \"gray30\"),         panel.grid.minor.x = element_line(linetype = \"dotted\",                                           color = \"gray80\")) +   labs(title = \"Epigraphic Evidence from Bithynia\",        subtitle = \"Spatio-temporal distribution\",        caption = attributes(inscriptions)$source) #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'"},{"path":"https://lsteinmann.github.io/datplot/articles/data_preparation.html","id":"how-does-datplot-and-kernel-density-estimation-perform-compared-to-histograms","dir":"Articles","previous_headings":"","what":"How Does Datplot and Kernel Density Estimation Perform Compared to Histograms?","title":"Data Preparation and Visualization","text":"order compare output generally common visualizations, can prepare histogram original data. two possible approaches. first prepare two-part histogram upper lower boundaries respective dating object. select three locations keep visualization short:  Alternatively, display mean Dating range:  also gives us impression distribution, seems information got lost way. Many inscriptions dated large time spans, now gathered mean value. upside histograms show real count inscriptions, density graphs . shown vignette, data set often encountered archaeologists can prepared formatted order processed datplot. Furthermore, made suggestions visualization. vignette hopes help researchers yet familiar processes data cleaning solutions projects.","code":"inscr_clean %>%    select(ID, Location, DAT_min, DAT_max) %>%   filter(Location %in% c(\"Prusias ad Hypium\", \"Nicomedia\", \"Apamea\")) %>%   reshape2::melt(id.vars = c(\"ID\", \"Location\")) %>%   ggplot(aes(x = value, fill = variable)) +   geom_histogram(binwidth = 25, position = \"dodge\") +   facet_wrap(. ~ Location, ncol = 1) +   labs(title = \"Distribution of Dated Inscriptions in Bithynia (selection)\",        x = \"Dating\", y = \"Number of Inscriptions\") +   theme(legend.position = \"top\") #> Warning: attributes are not identical across measure variables; they will be #> dropped inscr_clean %>%    transmute(ID, Location, Language, DAT_mean = ((DAT_min + DAT_max)/2)) %>%   filter(Location %in% c(\"Prusias ad Hypium\", \"Nicomedia\", \"Apamea\")) %>%   reshape2::melt(id.vars = c(\"ID\", \"Location\", \"Language\")) %>%   ggplot(aes(x = value, fill = Language)) +   geom_histogram(binwidth = 25) +   facet_wrap(. ~ Location, ncol = 1) +   labs(title = \"Distribution of Dated Inscriptions in Bithynia (selection)\",        x = \"Dating\", y = \"Number of Inscriptions\")"},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"why","dir":"Articles","previous_headings":"","what":"Why?","title":"Density Plots for Dates","text":"rather common problem archaeology fuzziness dates assigned objects. one wants visualize overall changes - let’s say - pottery consumption, bar charts often fall short regard. Phases – f, objects can usually dated , c, f, example, others classified “c” “b c”. can data still used examining changes large set objects, respecting quality dating? answer lies aoristic analysis, implemented criminology assess general temporal occurrence burglaries reported time span individual burglaries happened (Ratcliffe 2000). concept implemented archaeology (see Orton, Morris, Pipe (2017), 3–5 recent overview; generally credited first implementation archaeology Johnson (2004)). R-packages implement aoristic analysis archaeology, usually less object-based focus differing algorithms (namely aoristAAR, archSeries, tabula rtefact). wholeheartedly recommend trying packages well find best fit data. implementation (datplot) kept rather simple. case study package published (Weissova Steinmann 2021) addresses classical archaeology, package way restricted discipline.","code":""},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"how","dir":"Articles","previous_headings":"","what":"How?","title":"Density Plots for Dates","text":"First, important translate phases numbers. easily possible archaeological data, usually accepted date phase period. directly providing numeric absolute chronological values, e.g. iDAI chronontology provides approaches unified terminology. Usually, depend discipline geographical frame research particular chosen analysis. concordance phases absolute dating exists, easy apply single objects. illustrated process cleaning transforming real world archaeological data manual solution “Data Preparation Visualization”-vignette package. Using may cause problems end, since phases often employed avoid dates, necessary aim visualize distribution (pseudo-)continuous scale, numbers needed. Also, step may reversed final visualization supplementing replacing scale x-axis respective phases. automated process implemented yet. Ideally, one can produce ‘beginning’ ‘end’ date object, let’s say earliest possible dating latest possible dating, e.g. corresponding beginning start phase object dated . show explain work, chose random sample Athenian pottery Beazley archive (Oxford n.d.), large publicly available data set. (Since format provided BAPD slightly different needed converted data beforehand match requirements. values changed. sample data set included datplot. recent version Inscriptions Bithynia data set added datplot well. cleaning formatting detailed “data_preparation”-vignette example scholars looking approaches transformation verbose heterogeneous data. vignettes also suggests visualizations said data set.)","code":"library(datplot) data(Beazley)"},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"how-to-display-a-range","dir":"Articles","previous_headings":"","what":"How to Display a Range?","title":"Density Plots for Dates","text":"table provides two dates object. earliest possible dating (DAT_min) latest possible dating (DAT_max). order able process density graph, elegant means visualization continuous distributions (see also Crema, Bevan, Lake (2010), 1123 Baxter Cool (2016), 125–126). (least goal merely evaluate changes time probability object groups dating certain point time, look actual objects counts, obscured.) scale use pseudo-continuous, data actually comprises intervals (1 year, 25 years, etc.), nonetheless treat continuous one. Objects can dated greater confidence larger impact overall visualization. core function package (datsteps()) produces column named either ‘weight’ ‘probability’ contains value reflects quality dating (‘weight’) probability object dated certain year (‘probability’). calculation ‘weight’, see (Weissova Steinmann 2021). formula probability calculation simply ‘1 / ((max - min) + 1)’. case, greater time span, lower weight value probability. generally accepted method aoristic analysis (Ratcliffe 2000, 671–72 Fig. 1) explained greater detail (Weissova Steinmann 2021). case stepsize 1, weight value can seen actual dating probability specific year. example, coin dated 36 37 CE thus probability 0.5 year 36, 0.5 year 37. calculation probability , however work greater stepsizes. possible switch variants calculation using datsteps(x, calc = \"weight\") datsteps(x, calc = \"probability\"). probability calculated, option add cumulative probability column (datsteps(x, calc = \"probability\", cumulative = TRUE)). Secondly, every object duplicated number times equal dating range divided stepsize-variable. duplicate ‘DAT_step’ – one single value two extremes. mentioned weight variable divided number steps, new fictional object ‘date step’ counts fraction actual object. , hope achieve greater influence closely dated objects higher emphasis overlap. method useful dating specific contexts, since concept terminus post/ante quem lost , important smaller scale. may however applicable contexts disturbed, noticeable overlap objects give indication original tpq-date. method suitable also visualization changes trends time, e.g. popularity pottery types, overall peaks occupation survey data (Orton, Morris, Pipe 2017, 5). approaches, e.g. using median date object, may cases produce similar outcomes, create problems. lot information lost way employing averaged median data, large amount loosely dated objects produce peaks unreasonable values. (Consider large amount objects dated 600 400 BCE attributed year 500 BCE.)  Employing dating steps even unreasonable peaks. Note especially gap -425 -300 plot , – plot – filled constant amount objects year. due data containing large amounts objects dating -400 -300 BCE. course, due duplicating object numerous times (see table ), counts represented y-axis now inform us maximum amount objects might dated given bin rather actual amount. method datplot uses partitioning steps explained follows: given object dated ‘475 425 BC’ steps 25 years used object, result creation three sub-objects respective dating : 475 BC, 450 BC, 425 BC. interval 5 years, object partitioned 11 sub-objects, dating 475 BC, 470 BC, 465 BC, 460 BC, 455 BC, 450 BC, 445 BC, 440 BC, 435 BC, 430 BC 425 BC respectively. order keep data comparable, interval (expressed stepsize-variable R-package) must chosen globally objects. time span object dated can many cases divided without remainder value given stepsize (.e. may divided modulus division greater 0), datplot resolves conflicts temporarily modifying size steps objects question without deviating significantly size set beforehand, output representative comparable. Objects dated time span undercutting stepsize-variable can either assigned one year (objects dated precisely one year), two years (objects dated number years less equal 40% stepsize) three years (objects dated number years exceeding 60% stepsize). Using larger step sizes recommended stepsize 1 year leads significant delays processing yielding additional information. one look total time span 10000 years, step sizes 1 longer seem reasonable rather necessary, detailed changes visible graspable , using steps 100 years lead outcome. classical archaeologists duration 1000 years mostly exceeds area study. seem get good results stepsizes 1 25.  datsteps() can also calculate stepsize . equals closest possible dating object. stepsize used saved attribute, can recycled dynamically chose binwidth histograms:  sample use encompasses objects dated resolution 50 years, best also proceed stepsize. smaller stepsize paint detailed picture, just uses computing time without benefit. Due impossibility displaying object counts well, ideal use kernel density estimates visualization. density plot shows result. peak around -500 indicates area highest overlay, large part objects sample dated around time. distribution can also seen bar plots . , however, yet informative.","code":"Beazley$DAT_mean <- (Beazley$DAT_max + Beazley$DAT_min) / 2 library(ggplot2) ggplot(Beazley, aes(x = DAT_mean, fill = Technique)) +   geom_histogram(binwidth = 25, position = \"dodge\") + Plot_Theme + Plot_Fill system.time(result <- datsteps(Beazley, stepsize = 25))[3] #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> elapsed  #>   0.046 system.time(result <- datsteps(Beazley, stepsize = 1))[3] #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> elapsed  #>   0.143 library(datplot) result <- datsteps(Beazley, stepsize = 25) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). ggplot(result, aes(x = DAT_step, fill = variable)) +   geom_histogram(binwidth = 25, position = \"dodge\") + Plot_Theme + Plot_Fill result <- datsteps(Beazley, stepsize = \"auto\") #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> [1] \"Using stepsize = 51 (auto).\" #> Warning in create.sub.objects(DAT_list, stepsize, calc, cumulative): stepsize #> is larger than the range of the closest dated object at Index = 1, 2, 3, 4, 5, #> 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, #> 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, #> 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, #> 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, #> 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, #> 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, #> 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, #> 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, #> 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 176, 177, 178, #> 179, 180, 181, 182, 183, 184, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, #> 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, #> 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, #> 229, 230, 232, 233, 234, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, #> 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, #> 264, 265, 266, 267, 269, 270, 271, 273, 275, 276, 277, 279, 280, 281, 282, 283, #> 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, #> 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 314, 315, 316, 317, 318, #> 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 335, 336, #> 337, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 351, 352, 353, 354, 355, #> 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, #> 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 387, 388, #> 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, #> 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, #> 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 433, 434, 436, 437, 438, 439, #> 440, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, #> 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, #> 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, #> 490, 491, 492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, #> 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, #> 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, #> 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, #> 555, 556, 557, 559, 560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 571, 572, #> 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, #> 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 605, 606, #> 607, 608, 609, 610, 611, 612, 613, 615, 616, 617, 618, 619, 620, 621, 622, 623, #> 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 638, 639, 640, #> 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 657, #> 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 674, #> 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 692, #> 693, 694, 695, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, #> 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, #> 728, 729, 730, 731, 732, 733, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, #> 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761, #> 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, #> 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, #> 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, #> 810, 811, 812, 813, 814, 815, 817, 818, 819, 820, 821, 822, 823, 825, 826, 827, #> 828, 829, 830, 831, 832, 833, 834, 835, 836, 838, 839, 840, 841, 842, 843, 844, #> 845, 846, 847, 848, 851, 852, 853, 855, 856, 857, 858, 859, 860, 861, 862, 863, #> 864, 865, 866, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 881, 882, #> 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, #> 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 913, 914, 915, 916, #> 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, #> 933, 934, 935, 936, 937, 938, 939, 940, 941, 943, 944, 945, 946, 947, 948, 949, #> 950, 951, 952, 953, 954, 955, 956, 958, 959, 961, 962, 963, 964, 965, 966, 967, #> 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 982, 983, 984, #> 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 998, 999, 1000). #> For information see documentation of get.step.sequence(). ggplot(result, aes(x = DAT_step, fill = variable)) + Plot_Theme + Plot_Fill +   geom_histogram(binwidth = attributes(result)$stepsize, position = \"dodge\") result <- datsteps(Beazley, stepsize = 25) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). dens <- result dens <- scaleweight(result, var = \"all\") dens <- density(x = dens$DAT_step, weights = dens$weight) #> Warning in density.default(x = dens$DAT_step, weights = dens$weight): Selecting #> bandwidth *not* using 'weights' plot(dens)"},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"scaling-the-weight-along-groups-of-objects","dir":"Articles","previous_headings":"","what":"Scaling the Weight along Groups of Objects","title":"Density Plots for Dates","text":"order display objects separated groups, weights first scaled along group membership, sum weights group equal 1. datplots function scaleweight() exactly dataframe returned datsteps(). column contains variables group membership needs indicated.","code":"result <- scaleweight(result, var = 2)"},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"plots-for-the-distribution-of-objects-across-time","dir":"Articles","previous_headings":"","what":"Plots for the Distribution of Objects across Time","title":"Density Plots for Dates","text":"case Beazley archives data (Oxford n.d.) can clearly see knew : Black-figure pottery older red-figure pottery. (data random sample athenian pottery Beazley archive, n = 1000.)  case data, changes showing omitting weight negligent. , however, case heterogeneously dated sets objects (see “Data Preparation Visualization”-vignette). Please note – even Beazley-data – plot, however little, change weights omitted:  every step valued equally, lot steps fall end 4th century (mentioned ), since dated e.g. “-400 -300”. impact huge, dating ranges objects vary greatly. However, differences can dramatic heterogeneous data. quickly illustrate can plot two versions density Inscriptions Bithynia data included package:","code":"ggplot(data = result, aes(x = DAT_step,                           fill = variable,                           weight = weight)) +   geom_density(alpha = 0.5) +   xlab(\"Dating\") + Plot_Theme + Plot_Fill ggplot(data = result, aes(x = DAT_step,                           fill = variable)) +   geom_density(alpha = 0.5) +   xlab(\"Dating\") + Plot_Theme + Plot_Fill data(\"Inscr_Bithynia\") Inscr_Bithynia <- na.omit(Inscr_Bithynia[, c(1, 3, 8, 9)]) result_bith <- scaleweight(datsteps(Inscr_Bithynia, stepsize = \"auto\"),                            var = \"all\") #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> [1] \"Using stepsize = 1 (auto).\"  ggplot(result_bith, aes(x = DAT_step)) + Plot_Theme + Plot_Fill +   geom_density(alpha = 0.5, fill = \"grey30\") + xlab(\"Dating\") ggplot(result_bith, aes(x = DAT_step, weight = weight)) +   Plot_Theme + Plot_Fill +   geom_density(alpha = 0.5, fill = \"grey30\") + xlab(\"Dating\")"},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"including-histograms-for-an-impression-of-quantity","dir":"Articles","previous_headings":"","what":"Including Histograms for an Impression of Quantity","title":"Density Plots for Dates","text":"also added function calculated value needed scale density curve axis Histogram dating steps. Please note histogram show actual objects counts, counts maximum possible objects dated corresponding year resp. bin. value scale density curve combined plot histogram can obtained via get.histogramscale:  combination density curve histogram also shows common problem histograms. output depends significantly first bin placed may show skewed distribution especially roughly dated objects. Additionally, histograms also depend greatly binwidth may display meaningful pattern small bins chosen (Shennan 1988, 25–26; Baxter Beardah 1996). smooth curves kernel density estimates realistic approach dating. production objects continuous use, seems reasonable display continuous fashion flexible timescale.","code":"histogramscale <- get.histogramscale(result) ggplot(result, aes(x = DAT_step, fill = variable)) + Plot_Theme + Plot_Fill +   stat_density(alpha = 0.5, position = \"dodge\",                aes(y = (after_stat(density) * histogramscale), weight = weight)) +   geom_histogram(alpha = 0.5, binwidth = attributes(result)$stepsize,                  position = \"dodge\") +   labs(y = \"maximum number of objects per year\", x = \"Dating\") #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights'  #> Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = #> kernel, : Selecting bandwidth *not* using 'weights' #> Warning: Width not defined #> ℹ Set with `position_dodge(width = ...)`"},{"path":"https://lsteinmann.github.io/datplot/articles/how-to.html","id":"weights-and-cumulative-weights","dir":"Articles","previous_headings":"","what":"Weights and Cumulative Weights","title":"Density Plots for Dates","text":"update v1.1.0 new option calculation. using stepsize 1 calc = \"probability\", weights now reflect true probability objects dating specific year. , probability may used indicative value helper visualization. Many thanks Christian Gugl suggestion. now opens possibility calculate cumulative probability successive year – also suggested Christian Gugl –, useful case numismatic research, reflects rising probability minting. cumulative probability introduced option v1.1.0 calculated separate column setting cumulative = TRUE datsteps()-function. Please note cumulative probability work (mean anything) stepsizes larger 1!","code":"data(\"Inscr_Bithynia\") Inscr_Bithynia <- na.omit(Inscr_Bithynia[, c(1, 3, 8, 9)]) Inscr_Bithynia <- Inscr_Bithynia[sample(seq_len(nrow(Inscr_Bithynia)), 5), ] Inscr_Bithynia_steps <- datsteps(Inscr_Bithynia,                                   stepsize = 1,                                   calc = \"probability\",                                   cumulative = TRUE) #> Using step-wise probability calculation.  ggplot(Inscr_Bithynia_steps, aes(x = DAT_step, y = cumul_prob, fill = variable)) +    geom_col() + facet_wrap(. ~ ID, ncol = 1) +   labs(y = \"Cumulative Probability\", x = \"Dating\", fill = \"Origin\") +    theme(legend.position = \"bottom\")"},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lisa Steinmann. Author, maintainer, copyright holder. Barbora Weissova. Contributor.","code":""},{"path":"https://lsteinmann.github.io/datplot/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Steinmann L (2023). datplot: Preparation Object Dating Ranges Density Plots (Aoristic Analysis). R package version 1.1.0,  https://lsteinmann.github.io/datplot/, https://github.com/lsteinmann/datplot.","code":"@Manual{,   title = {datplot: Preparation of Object Dating Ranges for Density Plots (Aoristic Analysis)},   author = {Lisa Steinmann},   year = {2023},   note = {R package version 1.1.0,  https://lsteinmann.github.io/datplot/},   url = {https://github.com/lsteinmann/datplot}, }"},{"path":"https://lsteinmann.github.io/datplot/index.html","id":"datplot","dir":"","previous_headings":"","what":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","title":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","text":"Converting date ranges dating ‘steps’ eases visualization changes e.g. pottery consumption, style variables time. package provides tools process prepare data visualization. rather common problem archaeology fuzzyness dates assigned objects. one wants visualize overall changes - let’s say - pottery consumption, bar charts often fall short regard. , e.g., Phases – f employed, objects can usually dated , c, f, example, others classified “c” “b c”. can data still used examining changes large set objects without completely disregarding information added providing multiple phases one object? package proposes implements concepts aoristic analysis prepare archaeological data visualization using density plots. example shown vignettes, can found pkgdown-site installing package, GitHub /vignettes/ directory. Density plots easy understand usually aesthetically pleasing. omit information, individual counts, bar histograms can communicate better. hand, ranges can incorporated visualization well regard variety timespans archaeological objects may dated . Attic Pottery BAPD Date","code":"browseVignettes(\"datplot\")"},{"path":"https://lsteinmann.github.io/datplot/index.html","id":"publication","dir":"","previous_headings":"","what":"Publication","title":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","text":"package version 1.0.0 published along case study inscriptions Bithynia: Steinmann, L., & Weissova, B. (2021). Datplot: New R Package Visualization Date Ranges Archaeology. Advances Archaeological Practice, 1-11. doi:10.1017/aap.2021.8. Data used case study included package.","code":""},{"path":"https://lsteinmann.github.io/datplot/index.html","id":"recommendation","dir":"","previous_headings":"","what":"Recommendation","title":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","text":"People interested employing method also consider taking look ISAAKiel’s package aoristAAR, archSeries, tabula, rtefact, aoristic-analysis (LimesLimits) (future) baorista.","code":""},{"path":"https://lsteinmann.github.io/datplot/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","text":"‘datplot’ can installed GitHub devtools: via downloading latest release installing file: case unable find vignettes installing github directly, try: may install vignette dependencies manually (see suggests DESCRIPTION). Anyone tidyverse installed encounter issues.","code":"devtools::install_github(\"lsteinmann/datplot\") devtools::install_local(path = \"../datplot_1.x.x.tar.gz\") devtools::install_github(\"lsteinmann/datplot\", build_vignettes = TRUE)"},{"path":"https://lsteinmann.github.io/datplot/index.html","id":"contact","dir":"","previous_headings":"","what":"Contact","title":"Preparation of Object Dating Ranges for Density Plots (Aoristic\n    Analysis)","text":"Please feel free use change code liking. happy feedback packages, notify us publications using package!","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Beazley.html","id":null,"dir":"Reference","previous_headings":"","what":"Beazley (sample of 1000) — Beazley","title":"Beazley (sample of 1000) — Beazley","text":"test dataset containing data.frame ideally arranged work datplot.Data gathered Beazley Archive Pottery Database (BAPD) -- https://www.carc.ox.ac.uk/carc/pottery transformed work datplot","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Beazley.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Beazley (sample of 1000) — Beazley","text":"","code":"data(Beazley)"},{"path":"https://lsteinmann.github.io/datplot/reference/Beazley.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Beazley (sample of 1000) — Beazley","text":"data frame 1000 rows 4 variables","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Beazley.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Beazley (sample of 1000) — Beazley","text":"https://www.carc.ox.ac.uk/carc/pottery","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Beazley.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Beazley (sample of 1000) — Beazley","text":"Identifier (Vase.Number BAPD) Technique: Sample contains red- blackfigured objects DAT_min. Integer: lower range dating, BCE negative numbers DAT_max. Integer: upper range dating, BCE negative numbers","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/DAT_df.html","id":null,"dir":"Reference","previous_headings":"","what":"datplot Testing data — DAT_df","title":"datplot Testing data — DAT_df","text":"test dataset containing data.frame ideally arranged work datplot. Data real illustrate common problems lower upper dating wrong columns.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/DAT_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"datplot Testing data — DAT_df","text":"","code":"data(DAT_df)"},{"path":"https://lsteinmann.github.io/datplot/reference/DAT_df.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"datplot Testing data — DAT_df","text":"data frame 5000 rows 4 variables","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/DAT_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"datplot Testing data — DAT_df","text":"ID. Identifier Objects (unique) var. Grouping variable, Type Findspot DAT_min. Integer: lower range dating, BCE negative numbers DAT_max. Integer: upper range dating, BCE negative numbers","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Inscr_Bithynia.html","id":null,"dir":"Reference","previous_headings":"","what":"Inscr_Bithynia — Inscr_Bithynia","title":"Inscr_Bithynia — Inscr_Bithynia","text":"data set gathered Barbora Weissova published part dissertation “Regional Economy, Settlement Patterns Road System Bithynia (4th Century BC - 6th Century AD). Spatial Quantitative Analysis.”.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Inscr_Bithynia.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inscr_Bithynia — Inscr_Bithynia","text":"","code":"Inscr_Bithynia"},{"path":"https://lsteinmann.github.io/datplot/reference/Inscr_Bithynia.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Inscr_Bithynia — Inscr_Bithynia","text":"data frame 2878 rows 9 variables: ID character COLUMN_DESCRIPTION ikey character ID https://inscriptions.packhum.org/   / https://edh-www.adw.uni-heidelberg.de/home, available Location factor Findspot Inscription (City) Source character Corpus/Citation Inscription Dating character Original Chronological Assessment,   may contain inconsistencies Language factor Language Inscription,   can either Latin, Greek, uncertain_dating logical TRUE Dating certain,   FALSE dating certain DAT_min integer lower border dating timespan,   negative values BCE, positive values CE DAT_max integer upper border dating timespan,   negative values BCE, positive values CE URL Link inscription (available)   https://inscriptions.packhum.org/   https://edh-www.adw.uni-heidelberg.de/home","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/Inscr_Bithynia.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Inscr_Bithynia — Inscr_Bithynia","text":"Weissova, Barbora. 2019. “Regional Economy, Settlement Patterns Road System Bithynia (4th Century BC - 6th Century AD). Spatial Quantitative Analysis.” Dissertation, Berlin: Freie Universität Berlin. https://refubium.fu-berlin.de/handle/fub188/23730, partially https://inscriptions.packhum.org/","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.number.html","id":null,"dir":"Reference","previous_headings":"","what":"Check for numbers (internal) — check.number","title":"Check for numbers (internal) — check.number","text":"Checks value either numeric, integer double returns TRUE.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check for numbers (internal) — check.number","text":"","code":"check.number(value)"},{"path":"https://lsteinmann.github.io/datplot/reference/check.number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check for numbers (internal) — check.number","text":"value value check","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.number.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check for numbers (internal) — check.number","text":"TRUE value kind number, FALSE value ","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.structure.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if the structure is compatible with [datsteps()]  (internal) — check.structure","title":"Check if the structure is compatible with [datsteps()]  (internal) — check.structure","text":"Checks object passed [datsteps()] can used processing.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.structure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if the structure is compatible with [datsteps()]  (internal) — check.structure","text":"","code":"check.structure(DAT_df)"},{"path":"https://lsteinmann.github.io/datplot/reference/check.structure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if the structure is compatible with [datsteps()]  (internal) — check.structure","text":"DAT_df object check","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/check.structure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if the structure is compatible with [datsteps()]  (internal) — check.structure","text":"TRUE object can processed [datsteps()], error / FALSE ","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/create.sub.objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Create sub-objects for each object in a dataframe (internal) — create.sub.objects","title":"Create sub-objects for each object in a dataframe (internal) — create.sub.objects","text":"Requires list named vectors [datsteps()] hand function.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/create.sub.objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create sub-objects for each object in a dataframe (internal) — create.sub.objects","text":"","code":"create.sub.objects(DAT_list, stepsize, calc = \"weight\", cumulative = FALSE)"},{"path":"https://lsteinmann.github.io/datplot/reference/create.sub.objects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create sub-objects for each object in a dataframe (internal) — create.sub.objects","text":"DAT_list list prepared [datsteps()] stepsize numeric, default 1. Number years used interval creating dating steps. calc method calculation use; can either one \"weight\" (default) \"probability\":  * \"weight\": use     [published original calculation](https://doi.org/10.1017/aap.2021.8)     weights,  * \"probability\": calculate year-wise probability instead (reasonable     `stepsize = 1`) cumulative FALSE (default), TRUE: add column containing cumulative probability object (reasonable `stepsize = 1`, automatically use probability calculation)","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/create.sub.objects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create sub-objects for each object in a dataframe (internal) — create.sub.objects","text":"expanded list structure processed [datsteps()] object duplicated according number steps required","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/datplot-package.html","id":null,"dir":"Reference","previous_headings":"","what":"datplot: Preparation of Object Dating Ranges for Density Plots (Aoristic Analysis) — datplot-package","title":"datplot: Preparation of Object Dating Ranges for Density Plots (Aoristic Analysis) — datplot-package","text":"Converting date ranges dating 'steps' eases visualization changes e.g. pottery consumption, style variables time. package provides tools process prepare data visualization employs concept aoristic analysis.","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/reference/datplot-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"datplot: Preparation of Object Dating Ranges for Density Plots (Aoristic Analysis) — datplot-package","text":"Maintainer: Lisa Steinmann lisa.steinmann@rub.de (ORCID) [copyright holder] contributors: Barbora Weissova barbora.weissova@rub.de (ORCID) [contributor]","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/datsteps.html","id":null,"dir":"Reference","previous_headings":"","what":"Create 'steps' of dates for each object in a data.frame — datsteps","title":"Create 'steps' of dates for each object in a data.frame — datsteps","text":"function transforms data.frame dated objects associated data new data.frame contains row dating 'step' objects. Dating 'steps' can single years (`stepsize = 1`) arbitrary number used guideline interval. expected dates BCE displayed negative values dates CE positive values. Ignoring cause problems. dates provided wrong order number rows automatically switched. function along guide use case study published [Steinmann -- Weissova 2021](https://doi.org/10.1017/aap.2021.8).","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/datsteps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create 'steps' of dates for each object in a data.frame — datsteps","text":"","code":"datsteps(DAT_df, stepsize = 1, calc = \"weight\", cumulative = FALSE)"},{"path":"https://lsteinmann.github.io/datplot/reference/datsteps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create 'steps' of dates for each object in a data.frame — datsteps","text":"DAT_df data.frame 4 variables:   * `ID` : identifier row, e.g. Inventory number (ideally character).   * `group` : grouping variable, type context (ideally factor).   * `DAT_min` : minimum dating (int/num), minimum dating boundary   single object, .e. earliest year object may dated .   * `DAT_min` : maximum dating (int/num), maximum dating boundary   single object, .e. latest year object may dated . columns _must_ order, column names irrelevant; row _must_ correspond one datable entity / object. stepsize numeric, default 1. Number years used interval creating dating steps. calc method calculation use; can either one \"weight\" (default) \"probability\":  * \"weight\": use     [published original calculation](https://doi.org/10.1017/aap.2021.8)     weights,  * \"probability\": calculate year-wise probability instead (reasonable     `stepsize = 1`) cumulative FALSE (default), TRUE: add column containing cumulative probability object (reasonable `stepsize = 1`, automatically use probability calculation)","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/datsteps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create 'steps' of dates for each object in a data.frame — datsteps","text":"expanded data.frame row represents dating 'step'. Added columns contain value step, 'weight' 'probability'- value step, (chosen) cumulative probability.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/datsteps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create 'steps' of dates for each object in a data.frame — datsteps","text":"","code":"data(\"Inscr_Bithynia\") DAT_df <- Inscr_Bithynia[, c(\"ID\", \"Location\", \"DAT_min\", \"DAT_max\")] DAT_df_steps <- datsteps(DAT_df, stepsize = 25) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> Warning: 1362 rows with NA-values in the dating columns will be omitted. #> Warning: Warning: DAT_min and DAT_max at Index: 60, 75, 141, 199, 214, 269, 334, 335, 347, 348, 625, 628, 637, 730, 879, 883, 884, 936, 939, 941, 942, 945, 949, 987, 988, 1022, 1023, 1028, 1029, 1046, 1047, 1048, 1052, 1053, 1057, 1059, 1114, 1172, 1185, 1188, 1189, 1256, 1499, 1500, 1502, 1514, 1515, 1516 have the same value! Is this correct? Please check the table for possible errors. #> Warning: stepsize is larger than the range of the closest dated object at Index = 6, 12, 14, 18, 19, 20, 21, 22, 39, 40, 41, 44, 45, 60, 74, 75, 76, 77, 79, 80, 83, 110, 113, 114, 125, 126, 131, 132, 133, 141, 145, 147, 154, 161, 162, 163, 167, 168, 171, 173, 174, 175, 194, 196, 198, 199, 201, 203, 204, 205, 206, 207, 208, 212, 214, 217, 218, 223, 231, 232, 233, 234, 235, 236, 237, 238, 240, 242, 243, 244, 245, 246, 265, 269, 271, 272, 273, 276, 292, 293, 323, 324, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 579, 582, 583, 584, 585, 617, 618, 619, 620, 621, 622, 623, 625, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 640, 646, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 935, 936, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 975, 976, 979, 980, 981, 982, 983, 984, 987, 988, 989, 1002, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1074, 1075, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1106, 1107, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1147, 1148, 1149, 1153, 1154, 1172, 1174, 1175, 1176, 1183, 1185, 1186, 1187, 1188, 1189, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1248, 1250, 1251, 1252, 1253, 1255, 1256, 1260, 1382, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1480, 1487, 1494, 1498, 1499, 1500, 1502, 1503, 1507, 1514, 1515, 1516). For information see documentation of get.step.sequence(). plot(density(DAT_df_steps$DAT_step))"},{"path":"https://lsteinmann.github.io/datplot/reference/generate.stepsize.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine stepsize (internal) — generate.stepsize","title":"Determine stepsize (internal) — generate.stepsize","text":"Determines stepsize selecting absolute minimum value upper lower end dating ranges.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/generate.stepsize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine stepsize (internal) — generate.stepsize","text":"","code":"generate.stepsize(DAT_mat)"},{"path":"https://lsteinmann.github.io/datplot/reference/generate.stepsize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine stepsize (internal) — generate.stepsize","text":"DAT_mat matrix prepared [datsteps()], resp. matrix witch columns names `datmin` `datmax` containing numeric/integer value dating ranges.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/generate.stepsize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine stepsize (internal) — generate.stepsize","text":"single numeric value can used minimal stepsize.","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/reference/get.histogramscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Scaling Factor for Combined Histogram Plots — get.histogramscale","title":"Scaling Factor for Combined Histogram Plots — get.histogramscale","text":"Requires data.frame produced [datsteps()] number DAT_df_steps. Calculates value y-axis density graph multiplied order visible corresponding histogram.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.histogramscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scaling Factor for Combined Histogram Plots — get.histogramscale","text":"","code":"get.histogramscale(DAT_df_steps, binwidth = \"stepsize\")"},{"path":"https://lsteinmann.github.io/datplot/reference/get.histogramscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scaling Factor for Combined Histogram Plots — get.histogramscale","text":"DAT_df_steps data.frame returned [datsteps()]. (also work single number vector.) binwidth bandwidth use density function histogram. equal stepsize used create data.frame. data.frame returned [datsteps()] given, stepsize can automatically assigned using corresponding attribute (`binwidth = \"stepsize\"`)","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.histogramscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scaling Factor for Combined Histogram Plots — get.histogramscale","text":"value scale density curve histogram plot visible","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.histogramscale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scaling Factor for Combined Histogram Plots — get.histogramscale","text":"","code":"data(\"Inscr_Bithynia\") DAT_df <- Inscr_Bithynia[, c(\"ID\", \"Location\", \"DAT_min\", \"DAT_max\")] DAT_df_steps <- datsteps(DAT_df, stepsize = 25) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> Warning: 1362 rows with NA-values in the dating columns will be omitted. #> Warning: Warning: DAT_min and DAT_max at Index: 60, 75, 141, 199, 214, 269, 334, 335, 347, 348, 625, 628, 637, 730, 879, 883, 884, 936, 939, 941, 942, 945, 949, 987, 988, 1022, 1023, 1028, 1029, 1046, 1047, 1048, 1052, 1053, 1057, 1059, 1114, 1172, 1185, 1188, 1189, 1256, 1499, 1500, 1502, 1514, 1515, 1516 have the same value! Is this correct? Please check the table for possible errors. #> Warning: stepsize is larger than the range of the closest dated object at Index = 6, 12, 14, 18, 19, 20, 21, 22, 39, 40, 41, 44, 45, 60, 74, 75, 76, 77, 79, 80, 83, 110, 113, 114, 125, 126, 131, 132, 133, 141, 145, 147, 154, 161, 162, 163, 167, 168, 171, 173, 174, 175, 194, 196, 198, 199, 201, 203, 204, 205, 206, 207, 208, 212, 214, 217, 218, 223, 231, 232, 233, 234, 235, 236, 237, 238, 240, 242, 243, 244, 245, 246, 265, 269, 271, 272, 273, 276, 292, 293, 323, 324, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 579, 582, 583, 584, 585, 617, 618, 619, 620, 621, 622, 623, 625, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 640, 646, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 935, 936, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 975, 976, 979, 980, 981, 982, 983, 984, 987, 988, 989, 1002, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1074, 1075, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1106, 1107, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1147, 1148, 1149, 1153, 1154, 1172, 1174, 1175, 1176, 1183, 1185, 1186, 1187, 1188, 1189, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1248, 1250, 1251, 1252, 1253, 1255, 1256, 1260, 1382, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1480, 1487, 1494, 1498, 1499, 1500, 1502, 1503, 1507, 1514, 1515, 1516). For information see documentation of get.step.sequence(). get.histogramscale(DAT_df_steps) #> [1] 343650  get.histogramscale(DAT_df_steps$DAT_step, binwidth = 20) #> [1] 274920 get.histogramscale(500, binwidth = 20) #> [1] 10000"},{"path":"https://lsteinmann.github.io/datplot/reference/get.probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the probability for each year and each dated object — get.probability","title":"Calculate the probability for each year and each dated object — get.probability","text":"Calculates probability object dated year / timeslot two vectors minimum maximum dating. Returns vector probabilities.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the probability for each year and each dated object — get.probability","text":"","code":"get.probability(DAT_min, DAT_max)"},{"path":"https://lsteinmann.github.io/datplot/reference/get.probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the probability for each year and each dated object — get.probability","text":"DAT_min numeric vector containing minimum date object DAT_max numeric vector containing maximum date object","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the probability for each year and each dated object — get.probability","text":"vector probabilities object dated single year within timespan (lesser value means object dated larger timespans, .e. less confidence).","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/reference/get.step.sequence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the sequence of dating steps — get.step.sequence","title":"Calculate the sequence of dating steps — get.step.sequence","text":"Produces appropriate sequence years minimum maximum dating. properly divided stepsize set beforehand, either three values generated objects dated range 60 objects dated timespan less equal 60 can divided without residual, normal sequence returned. residual, stepsize modified depending large residual .","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.step.sequence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the sequence of dating steps — get.step.sequence","text":"","code":"get.step.sequence(datmin = 0, datmax = 100, stepsize = 25)"},{"path":"https://lsteinmann.github.io/datplot/reference/get.step.sequence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the sequence of dating steps — get.step.sequence","text":"datmin numeric value minimum dating one object datmax numeric value maximum dating one object stepsize stepsize used","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.step.sequence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the sequence of dating steps — get.step.sequence","text":"sequence steps created [create.sub.objects()]","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/reference/get.step.sequence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the sequence of dating steps — get.step.sequence","text":"","code":"min_year <- -494 max_year <- -334 sequence <- get.step.sequence(datmin = min_year, datmax = max_year, stepsize = 25) print(sequence) #> [1] -494 -467 -442 -417 -392 -367 -334  min_year <- 1 max_year <- 100 sequence <- get.step.sequence(datmin = min_year, datmax = max_year, stepsize = 25) print(sequence) #> [1]   1  21  41  60  80 100"},{"path":"https://lsteinmann.github.io/datplot/reference/get.weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the weights for each dated object — get.weights","title":"Calculate the weights for each dated object — get.weights","text":"Calculates weights two vectors minimum maximum dating object. Returns dataframe weight first column FALSE second two rows value min max dating. See [publication](https://doi.org/10.1017/aap.2021.8) information calculated.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the weights for each dated object — get.weights","text":"","code":"get.weights(DAT_min, DAT_max)"},{"path":"https://lsteinmann.github.io/datplot/reference/get.weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the weights for each dated object — get.weights","text":"DAT_min numeric vector containing minimum date object DAT_max numeric vector containing maximum date object","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/get.weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the weights for each dated object — get.weights","text":"vector 'weight'-values datsteps-data.frame, quantification well object dated (lesser value means object dated larger timespans, .e. less confidence)","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/reference/scaleweight.html","id":null,"dir":"Reference","previous_headings":"","what":"Scales the content of a column — scaleweight","title":"Scales the content of a column — scaleweight","text":"Requires data.frame one variable one value column.","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/scaleweight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scales the content of a column — scaleweight","text":"","code":"scaleweight(DAT_df, var = c(\"all\", 2), val = 5)"},{"path":"https://lsteinmann.github.io/datplot/reference/scaleweight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scales the content of a column — scaleweight","text":"DAT_df data.frame var index name column used group variable, \"\" val index name column scaled (numeric)","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/scaleweight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scales the content of a column — scaleweight","text":"data.frame, scaled values specified column","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/scaleweight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scales the content of a column — scaleweight","text":"","code":"data(\"Inscr_Bithynia\") DAT_df <- Inscr_Bithynia[, c(\"ID\", \"Location\", \"DAT_min\", \"DAT_max\")] DAT_df_steps <- datsteps(DAT_df, stepsize = 25) #> Using 'weight'-calculation (see https://doi.org/10.1017/aap.2021.8). #> Warning: 1362 rows with NA-values in the dating columns will be omitted. #> Warning: Warning: DAT_min and DAT_max at Index: 60, 75, 141, 199, 214, 269, 334, 335, 347, 348, 625, 628, 637, 730, 879, 883, 884, 936, 939, 941, 942, 945, 949, 987, 988, 1022, 1023, 1028, 1029, 1046, 1047, 1048, 1052, 1053, 1057, 1059, 1114, 1172, 1185, 1188, 1189, 1256, 1499, 1500, 1502, 1514, 1515, 1516 have the same value! Is this correct? Please check the table for possible errors. #> Warning: stepsize is larger than the range of the closest dated object at Index = 6, 12, 14, 18, 19, 20, 21, 22, 39, 40, 41, 44, 45, 60, 74, 75, 76, 77, 79, 80, 83, 110, 113, 114, 125, 126, 131, 132, 133, 141, 145, 147, 154, 161, 162, 163, 167, 168, 171, 173, 174, 175, 194, 196, 198, 199, 201, 203, 204, 205, 206, 207, 208, 212, 214, 217, 218, 223, 231, 232, 233, 234, 235, 236, 237, 238, 240, 242, 243, 244, 245, 246, 265, 269, 271, 272, 273, 276, 292, 293, 323, 324, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 579, 582, 583, 584, 585, 617, 618, 619, 620, 621, 622, 623, 625, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 640, 646, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 935, 936, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 975, 976, 979, 980, 981, 982, 983, 984, 987, 988, 989, 1002, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1074, 1075, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1106, 1107, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1147, 1148, 1149, 1153, 1154, 1172, 1174, 1175, 1176, 1183, 1185, 1186, 1187, 1188, 1189, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1248, 1250, 1251, 1252, 1253, 1255, 1256, 1260, 1382, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1480, 1487, 1494, 1498, 1499, 1500, 1502, 1503, 1507, 1514, 1515, 1516). For information see documentation of get.step.sequence(). DAT_df_scaled <- scaleweight(DAT_df_steps, var = 2, val = 5)"},{"path":"https://lsteinmann.github.io/datplot/reference/switch.dating.html","id":null,"dir":"Reference","previous_headings":"","what":"Switch values where dating is in wrong order (internal) — switch.dating","title":"Switch values where dating is in wrong order (internal) — switch.dating","text":"Requires data.frame 2 numeric variables 3rd 4th column: minimum date (int/numeric) maximum date (int/numeric) used [datsteps()].","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/switch.dating.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Switch values where dating is in wrong order (internal) — switch.dating","text":"","code":"switch.dating(DAT_df)"},{"path":"https://lsteinmann.github.io/datplot/reference/switch.dating.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Switch values where dating is in wrong order (internal) — switch.dating","text":"DAT_df data.frame 4 variables order: ID, group, minimum date (int/num), maximum date (int/num)","code":""},{"path":"https://lsteinmann.github.io/datplot/reference/switch.dating.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Switch values where dating is in wrong order (internal) — switch.dating","text":"data.frame dating values wrong order switched.","code":""},{"path":[]},{"path":"https://lsteinmann.github.io/datplot/news/index.html","id":"datplot-110","dir":"Changelog","previous_headings":"","what":"datplot 1.1.0","title":"datplot 1.1.0","text":"Using either original calculation (weights) calculation year-wise probability now option datsteps() argument calc = \"weight\" calc = \"probability\" now option calculate cumulative probability datsteps() argument cumulative = TRUE. works probability calculation instead original (weights) calculation. Change improve error-handling scaleweight(). Remove UTF-8 characters comply CRAN. Update documentation add pkgdown-site.","code":""},{"path":"https://lsteinmann.github.io/datplot/news/index.html","id":"datplot-101","dir":"Changelog","previous_headings":"","what":"datplot 1.0.1","title":"datplot 1.0.1","text":"Change calculation get.weights() 1 / (abs(DAT_min - DAT_max) + 1) get real probability values year. real effect using stepsize 1, makes weight-values usable “dating probability”. Clean calculate.outputrows() scaleweight() somewhat.","code":""},{"path":"https://lsteinmann.github.io/datplot/news/index.html","id":"datplot-100","dir":"Changelog","previous_headings":"","what":"datplot 1.0.0","title":"datplot 1.0.0","text":"CRAN release: 2021-03-04 Added NEWS.md file track changes package style corrections First release submission CRAN, accepted -> datplot now CRAN","code":""},{"path":"https://lsteinmann.github.io/datplot/news/index.html","id":"datplot-024","dir":"Changelog","previous_headings":"","what":"datplot 0.2.4","title":"datplot 0.2.4","text":"peer-review version Advances Archaeological Practice","code":""}]

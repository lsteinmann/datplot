---
title: "datplot - Density Plots for Dates"
author: "Lisa Steinmann"
date: "28 Mai 2018"
output:
  pdf_document: default
  html_document: default
bibliography: ../inst/literatur.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```


## Density Plots for Dates

A rather common problem in archaeology is the fuzziness of dates assigned to objects. If one wants to visualize overall changes in - let's say - pottery consumption, bar charts often fall short in that regard. Let's say we have Phases a -- f, then some of the objects can usually be dated to a, c, and f, as an example, but others will by classified as "a to c" or "b to c". But one can these data still be used for examining changes in a large set of objects?

First, it is handy to translate the phases into numbers, for which we should conveniently choose the 'actual' dating. This may cause other problems in the end, since such phases are often employed to avoid dates, but it is necessary as the aim is to visualize the distribution on a continuous scale, for which numbers are needed. This step may later be reversed for the final visualization but supplementing or replacing the scale on the x-axis with the respective phases. 
Ideally, one can produce a 'beginning' and 'end' date for each object, or let's say an earliest possible dating and a latest possible dating corresponding to beginning and start of each phase the object is dated to. 

To show and explain how this would work, I chose a random sample of athenian pottery from the beazley archive ([@BAPD]), as it is a large publicly available dataset. (Since the format provided by the BAPD is slightly different from that needed here I converted the data beforehand to match my requirements. No values have been changed.)


```{r prep}
df <- read.table("../inst/data/testset_beazley_1000.csv", sep = ";")
kable(df[sample(1:nrow(df), 10, replace = FALSE),])
```


## How to display a range?

The table provides two dates for each object. The earliest possible dating (DAT_min) and the latest possible dating (DAT_max). In order to be able to process this to a density graph, which is the most elegant means of visualization for continuous distributions. (At least if the goal is merely to evaluate changes over time and not to look at actual objects counts, which will be omitted.)

Objects that can be dated with greater confidence should have a larger impact on the overall visualisation. That is why the following function productes a column named 'weight' which contains a value that corresponds to one (as the closest possible dating of one year) devided by the timespan between the two dating variables. The greater the timespan, the lower the weight value. 
Secondly, every object is duplicated a number of times equal to the dating range devided by the stepsize-variable. Each duplicate has its own 'date', one single value between the two extremes. The above mention weight variable is devided by the number of steps, so that each new fictional object or 'date step' counts only as a fraction of the actual object. 
In this case the size of the dating steps will be 10 years. 


I find this superior to using, e.g., the median date of an objects. The outcome might often be similar, but I still have the impression that a lot of information is lost on the way when doing that. I do not claim this to be an ideal solution to everything, however. There _are_ still problems and it might not be suitable for every purpose. I can't imagine this being useful for 'fine'-dating context, since it does erase the point of terminus post/ante quem, which is important on a smaller scale. I would rather imagine it being usefull to display the change in 'trends' over time, e.g. which vessel type is popular at a given time? How does style develop? Or maybe even to find occupation 'peaks' from survey data?


```{r steps}
library(datplot)
result <- datsteps(df, stepsize = 10)
kable(head(result))
```


This can now be displayed as a density plot, as seen below. The peak at around -500 indicates that is area has the highest overlay, so a large part of the objects in our sample have been dated around this time. This, however, is not yet very informative. 


```{r density one}
dens <- result
dens$weight <- (dens$weight / sum(dens$weight))
dens <- density(x = dens$DAT_Step, weights = dens$weight)
plot(dens)
```


A simplistic approach to a bar plot however whould look like this: 


```{r barplot}
counts <- df
counts$DAT_med <- ((counts$DAT_max + counts$DAT_min) / 2)
counts <- table(counts$DAT_med)
plot(counts)
```


## Scaling the weight along groups of objects

But: In order to display the objects seperated into groups, the weights first have to be scaled along group membership, so that the sum of all weights in a group will equal 1. scaleweight() does exactly that for a dataframe as it was returned by datsteps().
We also need to supply the group column.


```{r scaleweight}
result <- scaleweight(result, result$Technique)
kable(head(result))
```


  
## Plots for the distribution of objects across time

This can now be plotted a little more nicely using ggplot2. We can clearly see now what we knew before: Black-figure pottery is older than red-figure pottery. (The data are from a random sample of athenian pottery from the beazley archive, n = 1000. Computation of the dating steps may not work with very, very large datasets, or simply take up a lot of time.)

```{r ggplot}
library(ggplot2)
ggplot(data = result, aes(x = DAT_Step, 
                          color = Technique, 
                          fill = Technique)) +
  geom_density(aes(weight = weight), alpha = 0.5) +
  xlab("Dating") +
  theme(panel.background = element_blank())
```


Please note that the plot does change when the weights are omitted. When every step is valued equally, a lot of steps fall into the end of the 4th century, since they were dated as e.g. "-400 to -300".


```{r ggplot wout weight, warning=FALSE}
ggplot(data = result, aes(x = DAT_Step, 
                          color = Technique, 
                          fill = Technique)) +
  geom_density(alpha = 0.5) +
  xlab("Dating") +
  theme(panel.background = element_blank())
```


## Barplots for comparison


```{r barplot two, warning=FALSE}
counts <- df
counts$DAT_med <- ((counts$DAT_max + counts$DAT_min) / 2)
counts <- table(counts$DAT_med, counts$Technique)
library(reshape2)
counts <- melt(counts)
ggplot(data = counts, aes(x = Var1, y = value, fill = Var2)) +
  geom_bar(stat = "identity", position = "dodge")
```


The smooth curves of kernel density estimates are a more realistic approach to dating. The production of objects was as continuous as their use, so it seems only reasonable to display it in a continuous fashion.


## References


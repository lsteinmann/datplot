---
title: "Density Plots for Dates"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Density Plots for Dates}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/literatur.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.align = "center"
)

library(knitr)
library(ggplot2)
require(ggplot2)
plot_theme <- theme(panel.background = element_blank(), 
        panel.grid.major = element_line(color = "grey60", linetype = "dashed"),
        panel.grid.minor = element_line(color = "grey80", linetype = "dashed"),
        legend.position = c(0.9,0.85), legend.background = element_rect(fill = "white", color = "grey60"))
plot_fill <- scale_fill_manual(name="Technique", values = c("gray30", "tomato3"))
```


## Why?

A rather common problem in archaeology is the fuzziness of dates assigned to objects. If one wants to visualize overall changes in - let's say - pottery consumption, bar charts often fall short in that regard. If we have Phases a -- f, then some of the objects can usually be dated to a, c, and f, as an example, but others will by classified as "a to c" or "b to c". But how can these data still be used for examining changes in a large set of objects?

First, it is handy to translate the phases into numbers, for which we should conveniently choose the 'actual' dating. This may cause other problems in the end, since such phases are often employed to avoid dates, but it is necessary as the aim is to visualize the distribution on a continuous scale, for which numbers are needed. Also, this step may be reversed for the final visualization but supplementing or replacing the scale on the x-axis with the respective phases. 
Ideally, one can produce a 'beginning' and 'end' date for each object, or let's say an earliest possible dating and a latest possible dating, e.g. corresponding to beginning and start of each phase the object is dated to. 

To show and explain how this would work, I chose a random sample of athenian pottery from the beazley archive ([@BAPD]), as it is a large publicly available dataset. (Since the format provided by the BAPD is slightly different from that needed here I converted the data beforehand to match my requirements. No values have been changed. The sample dataset is included in datplot.)

```{r prep}
library(datplot)
data(Beazley)
```
```{r preptable, echo = FALSE}
kable(Beazley[sample(1:nrow(Beazley), 10, replace = FALSE),])
```

## How to display a range?

The table provides two dates for each object. The earliest possible dating (DAT_min) and the latest possible dating (DAT_max). In order to be able to process this to a [density graph](https://en.wikipedia.org/wiki/Kernel_density_estimation), which is the most elegant means of visualization for continuous distributions. (At least if the goal is merely to evaluate changes over time and not to look at actual objects counts, which will be omitted.)

Objects that can be dated with greater confidence should have a larger impact on the overall visualisation. The core function of this package (`datsteps()`) produces a column named 'weight' which contains a value that corresponds to one (as the closest possible dating of one year) devided by the timespan between the two dating variables. The greater the timespan, the lower the weight value. 
Secondly, every object is duplicated a number of times equal to the dating range devided by the stepsize-variable. Each duplicate has its own 'date', one single value between the two extremes. The above mention weight variable is devided by the number of steps, so that each new fictional object or 'date step' counts only as a fraction of the actual object. 

This method will not be useful for dating specific context, since any concept of _terminus post/ante quem_ is lost here, which is important on a smaller scale. For the visualization of changes in _trends_ over time, e.g. the popularity of pottery types, or overall peaks in occupation from survey data, the method is ideal. 

Other approaches, e.g. using the median date of each object, may often produce similar outcomes, but create other problems. A lot of information is lost on the way when employing averaged or median data, as large amount of loosely dated objects will produce peaks at unreasonable values. (Consider a large amount of objects dated between 600 and 400 BCE all attributed to the year 500 BCE.)

```{r barplot}
Beazley$DAT_mean <- (Beazley$DAT_max + Beazley$DAT_min) / 2
library(ggplot2)
ggplot(Beazley, aes(x = DAT_mean, fill = Technique)) + 
  geom_histogram(binwidth = 25, position = "dodge") + plot_theme + plot_fill
```

Employing dating steps will even out unreasonable peaks. Not especially the gap between -425 and -300 in the plot above, that is -- in the plot below -- filled with a constant amount of objects in each year. This is due to the data containing large amounts of objects dating from -400 to -300 BCE. Of couse, due to duplicating each object numerous times (see table below), the counts represented on the y-axis have become meaningless.

```{r steps1}
library(datplot)
result <- datsteps(Beazley, stepsize = 5)
ggplot(result, aes(x = DAT_step, fill = variable)) + 
  geom_histogram(binwidth = 25, position = "dodge") + plot_theme + plot_fill
```

`datsteps()` can also calculate a stepsize on its own. It equals the closest possible dating of any object. 

```{r steps2}
result <- datsteps(Beazley, stepsize = "auto")
ggplot(result, aes(x = DAT_step, fill = variable)) + 
  geom_histogram(binwidth = 25, position = "dodge") + plot_theme + plot_fill
```


```{r stepstable, echo = FALSE}
kable(head(result))
```

Due to the impossibility of displaying object counts as well, it is ideal to use kernel density estimates for visualization. The density plot below shows the result. The peak at around -500 indicates that is area has the highest overlay, so a large part of the objects in our sample have been dated around this time. The same distribution can also be seen in the bar plots above. This, however, is not yet very informative. 

```{r density one}
result <- datsteps(Beazley, stepsize = 25)
dens <- result
dens <- scaleweight(result, var = "all")
dens <- density(x = dens$DAT_step, weights = dens$weight)
plot(dens)
```



## Scaling the weight along groups of objects

In order to display the objects seperated into groups, the weights first have to be scaled along group membership, so that the sum of all weights in a group will equal 1. datplots function `scaleweight()` does exactly that for a dataframe as it was returned by `datsteps()`. A column that contains the variables for group membership needs to indicated.


```{r scaleweight}
result <- scaleweight(result, var = 2)
```

```{r scaleweighttable, echo = FALSE}
kable(head(result))
```


  
## Plots for the distribution of objects across time

In the case of the beazley archives data [@BAPD] we can clearly see what we knew before: Black-figure pottery is older than red-figure pottery. (The data are from a random sample of athenian pottery from the beazley archive, n = 1000. Computation of the dating steps may not work with very, very large datasets, or simply take up a lot of time.)

```{r ggplot, warning=FALSE}
ggplot(data = result, aes(x = DAT_step, 
                          fill = variable,
                          weight = weight)) +
  geom_density(alpha = 0.5) +
  xlab("Dating") + plot_theme + plot_fill
```


Please note that the plot does change when the weights are omitted (see plot below). When every step is valued equally, a lot of steps fall into the end of the 4th century (as mentioned above), since they were dated as e.g. "-400 to -300". The impact here is not very huge, as the dating ranges of the objects do not vary greatly. However, the differences can be very dramatic for more heterogenious data.

```{r ggplot without weight, warning=FALSE}
ggplot(data = result, aes(x = DAT_step, 
                          fill = variable)) +
  geom_density(alpha = 0.5) +
  xlab("Dating") + plot_theme + plot_fill
```

We also added a function that calculated the value needed to scale the density curve to the axis of a Histogram of the dating steps. Please note that the histogram will not show the actual objects counts, but the counts of the maximum possible objects dated to the corresponding year resp. bin. The value to scale the density curve for a combined plot with a histogram can be obtained via `get.histogramscale`: 


```{r histogramscale, warning = FALSE, message = FALSE}
histogramscale <- get.histogramscale(result)
```


```{r ggplot-combination}
ggplot(result, aes(x = DAT_step, fill = variable)) +
  stat_density(alpha = 0.5, position = "dodge", aes(y = (..density..*histogramscale), weight = weight ))+
  geom_histogram(alpha = 0.5, binwidth = 25, position = "dodge") + plot_theme + plot_fill +
  labs(y = "maximum number of objects per year", x = "Dating")
```

The combination of density curve and histogram also shows the common problem of histograms. Their output depends significantly on where the first bin is placed and may show a skewed distribution especially for roughly dated objects.

The smooth curves of kernel density estimates are a more realistic approach to dating. The production of objects was as continuous as their use, so it seems only reasonable to display it in a more continuous fashion on a flexible timescale.


## References

